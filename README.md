# ğŸµ InstruNet AI  
### Music Instrument Recognition System using Deep Learning

InstruNet AI is a Flask-based web application that recognizes musical instruments from audio files using a deep learning model (ResNet18). The system analyzes audio, generates Mel spectrograms, predicts instrument probabilities, and provides downloadable reports.

---

## ğŸš€ Features

- ğŸ¼ Upload audio files (.wav, .mp3, .flac)
- ğŸ§  Deep Learning based instrument classification
- ğŸ“Š Top 5 predicted instruments with probabilities
- ğŸ“ˆ Instrument probability timeline visualization
- ğŸ› Real-time waveform visualization
- ğŸ–¼ Mel Spectrogram generation
- ğŸ“¥ Download Mel Spectrogram as PNG
- ğŸ“„ Export full analysis report as PDF
- ğŸ•˜ Session-based analysis history (no duplicates)
- ğŸ¨ Modern responsive UI design

---
## ğŸ“‚ Dataset

This project is trained and evaluated using the **IRMAS (Instrument Recognition in Musical Audio Signals)** dataset.

- ğŸµ 11 Instrument Classes
- ğŸ§ 6,705 Training Samples
- ğŸ§ª 2,874 Test Samples
- ğŸ¼ 3-second audio excerpts

ğŸ”— Dataset Link:
https://www.upf.edu/web/mtg/irmas

---

## ğŸ— System Architecture

1. User uploads audio file  
2. Audio is converted into Mel Spectrogram  
3. Spectrogram resized to 224Ã—224  
4. Passed through ResNet18 model  
5. Softmax probabilities generated  
6. Top predictions displayed  
7. Results stored in session history  

---

## ğŸ§  Model Details

- Architecture: ResNet18  
- Input: Mel Spectrogram (128 Mel bands)  
- Image Size: 224 Ã— 224  
- Classes: 11 Instruments  
- Framework: PyTorch  
- Dataset: IRMAS  

### Supported Instruments

- Cello  
- Clarinet  
- Flute  
- Acoustic Guitar  
- Electric Guitar  
- Organ  
- Piano  
- Saxophone  
- Trumpet  
- Violin  
- Voice  

---

## ğŸ“‚ Project Structure
```
InstruNet-AI/
â”‚
â”œâ”€â”€ model/
â”‚ â””â”€â”€ best_resnet18_irmas.pth
â”‚
â”œâ”€â”€ templates/
â”‚ â”œâ”€â”€ home.html
â”‚ â”œâ”€â”€ analysis.html
â”‚ â”œâ”€â”€ results.html
â”‚ â”œâ”€â”€ history.html
â”‚ â””â”€â”€ about.html
â”‚
â”œâ”€â”€ static/
â”‚ â””â”€â”€ style.css
â”‚
â”œâ”€â”€ uploads/
â”œâ”€â”€ exports/
â”‚
â””â”€â”€ app.py
```

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone Repository

git clone https://github.com/YashHaval/InstruNet-AI.git

cd instrunet-ai


### 2ï¸âƒ£ Create Virtual Environment


python -m venv env
env\Scripts\activate # Windows


### 3ï¸âƒ£ Install Dependencies


pip install -r requirements.txt


### Required Libraries

- Flask
- PyTorch
- torchvision
- librosa
- matplotlib
- numpy
- reportlab

### 4ï¸âƒ£ Run Application

python app.py



---

## ğŸ“Š How It Works

- Audio is loaded at 22,050 Hz
- Split into 3-second windows with 1-second hop
- Silent segments are filtered
- Mel Spectrogram generated (128 bands)
- Spectrogram normalized
- Resized to 224Ã—224 for CNN input
- Softmax probabilities computed
- Top 5 instruments displayed
- Timeline plotted from segment probabilities

---

## ğŸ“„ PDF Export Includes

- File Name  
- Predicted Instrument  
- Confidence Percentage  
- Generated Timestamp  
- Top 5 Instruments Table  
- Model Information  
- Mel Spectrogram Image  

---

## ğŸ” Notes

- Session-based history (stored temporarily)
- Duplicate song entries automatically replaced
- Maximum upload size: 20MB
- Minimum audio duration: 1 second

---

## ğŸŒ Technologies Used

- Python
- Flask
- PyTorch
- Librosa
- Matplotlib
- ReportLab
- HTML / CSS / JavaScript

---

## ğŸš€ Future Improvements

- ğŸ™ Add real-time microphone input for live instrument detection
- ğŸ³ Deploy the application using Docker for easy scalability
- ğŸ—„ Add database integration (SQLite/PostgreSQL) for persistent history
- ğŸ¼ Support multi-label prediction for overlapping instruments
- ğŸ“Š Improve model accuracy using data augmentation techniques
- ğŸŒ Deploy to cloud platforms (AWS / Render / Railway)
- ğŸ“± Improve UI responsiveness for mobile devices


